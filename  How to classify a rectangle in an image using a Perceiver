{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" How to classify a rectangle in an image using a Perceiver","provenance":[{"file_id":"1_fRVoj4dKbP6Nf7Dn3gSS2TzWUknvhfH","timestamp":1617264489114},{"file_id":"1rCZWPpFlgPZC_sqiUtKRSf16rScJi0JW","timestamp":1617234405395}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MM9zInqygXLg"},"source":["## Instalation"]},{"cell_type":"markdown","metadata":{"id":"lYRPVFghqiI1"},"source":["Here we show how to use a Perciever [1] to do a very simple object classification task where we have a rectangle of a random length and width in a 64 pixel square and we classify where the rectangle is. First we have to install the perciever-pytorch implementation so execute the below cell.\n","\n"]},{"cell_type":"code","metadata":{"id":"1EmMgQSSra9M"},"source":["!pip install perceiver-pytorch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"05AF7S2o6SIS"},"source":["The below packages tga we import consist of OpenCV which helps us generate the synthetic data that we will use below. We also use pyTorch but the package name is called torch and we use that to both train and run the model. There is a specific import to display an image using google colab."]},{"cell_type":"code","metadata":{"id":"YNCh_-_1rhPX"},"source":["import time\n","import torch\n","import cv2\n","import numpy as np\n","import torch.optim as optim\n","import torchvision\n","\n","from google.colab.patches import cv2_imshow\n","from perceiver_pytorch import Perceiver\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8acaPf130W6E"},"source":["## Configuration\n","\n","Here we set a set of parameters \n"," that we will use to generate a synthetic dataset and pass these values to hyperparemetrs when we train and evalulate the model. The batch size is set to 128 and the size in pixels for each object which is a square with dimentions 64 by 64 pixels.\n"]},{"cell_type":"code","metadata":{"id":"DM59RbXg0U1v"},"source":["##################\n","batch_size = 128\n","size = 64\n","objects = 1\n","##################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OOOFkeI8hQA6"},"source":["#Hyperparameters \n","\n","The most interesting hyperparameters defined here are setting the number of classes to four and the input axis to two sice we are inputting pictures."]},{"cell_type":"code","metadata":{"id":"Kn5CTHlhrmdM"},"source":["model = Perceiver(\n","    input_channels = 3,          # number of channels for each token of the input\n","    input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n","    num_freq_bands = 4,          # number of freq bands, with original value (2 * K + 1)\n","    max_freq = 5.,               # maximum frequency, hyperparameter depending on how fine the data is\n","    depth = 2,                   # depth of net\n","    num_latents = 32,            # number of latents, or induced set points, or centroids. different papers giving it different names\n","    cross_dim = 32,              # cross attention dimension\n","    latent_dim = 32,             # latent dimension\n","    cross_heads = 1,             # number of heads for cross attention. paper said 1\n","    latent_heads = 4,            # number of heads for latent self attention, 8\n","    cross_dim_head = 32,\n","    latent_dim_head = 32,\n","    num_classes = 4*objects,     # output number of classes\n","    attn_dropout = 0.5,\n","    ff_dropout = 0.5,\n","    weight_tie_layers = True     # whether to weight tie layers (optional, as indicated in the diagram)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Fa3uMk_tNmx"},"source":["#A simple object detection problem.\n","\n","What we do here is synthesize a very simple object detection dataset where we have a image that is 64 by 64 pixels and then we draw a rectangle with a random color, width and length."]},{"cell_type":"code","metadata":{"id":"c2WaA0Vhrqhv"},"source":["\n","class CustomDataset(Dataset):\n","    def __init__(self):   \n","      pass\n","      \n","    def __len__(self):\n","        return 320000\n","\n","    def __getitem__(self, idx):\n","        image = np.zeros((size,size,3), np.uint8)\n","        labels = []\n","\n","        for i in range(objects):\n","          if (np.random.rand() > 0.02):\n","            point_x = int(np.random.rand() * size)\n","            point_y = int(np.random.rand() * size)\n","            size_x = int(np.random.rand() * size)\n","            size_y = int(np.random.rand() * size)       \n","            r, g, b = int(np.random.rand()*255), int(np.random.rand()*255), int(np.random.rand() * 255)  \n","            try:\n","              cv2.rectangle(image, (int(point_x - size_x/2), int(point_y - size_y/2)), (int(point_x + size_x/2), int(point_y + size_y/2)), (r, g, b), -1)            \n","              labels.append((point_x/size, point_y/size, size_x/size, size_y/size))\n","            except Exception as e:\n","              print(e)\n","              labels.append((0,0,0,0))\n","          else:            \n","            labels.append((0,0,0,0))            \n","                  \n","        labels = torch.as_tensor(labels, dtype=torch.float32)\n","        image = torch.as_tensor(image, dtype=torch.float32)\n","\n","        return (image, labels)\n","\n","\n","\n","criterion = torch.nn.MSELoss().cuda()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)\n","\n","dataset = CustomDataset()\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n","\n","model.cuda()\n","model.train()\n","\n","\n","for epoch in range(20):  # loop over the dataset multiple times\n","\n","  running_loss = 0.0\n","\n","  for i, batch in enumerate(dataloader):\n","\n","    img, labels = batch    \n","        \n","    img = torch.as_tensor(img, dtype=torch.float32).cuda()\n","\n","    labels = torch.as_tensor(labels, dtype=torch.float32).cuda()    \n","    labels = labels.flatten()\n","    labels = torch.reshape(labels, (batch_size, 4*objects))\n","     \n","    optimizer.zero_grad()\n","    out = model(img)\n","        \n","   \n","    loss = criterion(out, labels)             \n","    loss.backward()\n","    optimizer.step()\n","\n","    # print statistics\n","    running_loss += loss.item()\n","    if i % 20 == 19:    # print every 2000 mini-batches\n","        print('[Epoch: %d, %6d] loss: %.6f' %\n","              (epoch + 1, i + 1, running_loss / 20))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zo5JKon3u3D0"},"source":["## Testing the trained object detection model.\n","Here we test the trained model. We show an example by classifiying an input image with a rectangle of a random color and length and placed randomly in the image and try to recognize where the rectangle is and thew width and length."]},{"cell_type":"code","metadata":{"id":"ovpU5I04r3mP"},"source":["model = model.eval()\n","\n","np.random.seed(np.random.randint(0, 99999))\n","\n","image = np.zeros((size,size, 3), np.uint8)\n","labels = []\n","model = model.cuda()\n","\n","point_x = int(np.random.rand() * size)\n","point_y = int(np.random.rand() * size)\n","size_x = int(np.random.rand() * size)\n","size_y = int(np.random.rand() * size)       \n","r, g, b = int(np.random.rand()*255), int(np.random.rand()*255), int(np.random.rand() * 255)  \n","\n","cv2.rectangle(image, (int(point_x - size_x/2), int(point_y - size_y/2)), (int(point_x + size_x/2), int(point_y + size_y/2)), (r, g, b), -1)            \n","\n","print(\"input image:\")\n","cv2_imshow(image)\n","print(\"input box location (x, y), (w, h):\")\n","print((int(point_x - size_x/2), int(point_y - size_y/2)), (int(point_x + size_x/2), int(point_y + size_y/2)))\n","\n","image_tensor = torch.as_tensor(image, dtype=torch.float32)\n","\n","image_tensor = torch.unsqueeze(image_tensor, 0).cuda()    \n","\n","image2 = np.ones((size,size, 3), np.uint8)\n","\n","t1 = time.time()\n","for i in range(1):\n","  out = model(image_tensor)\n","  out = out[0].detach().cpu().numpy()\n","  \n","out_point_x = int(out[0] * size)\n","out_point_y = int(out[1] * size)\n","out_size_x = int(out[2] * size)\n","out_size_y = int(out[3] * size)\n","\n","cv2.rectangle(image2, (int(out_point_x - out_size_x/2), int(out_point_y - out_size_y/2)), (int(out_point_x + out_size_x/2), int(out_point_y + out_size_y/2)), (r, g, b), -1)            \n","\n","print(\"-\"*50)\n","print(\"predicted output:\")\n","cv2_imshow(image2)\n","print(\"predicted box location (x, y), (w, h):\")\n","print((int(out_point_x - out_size_x/2), int(out_point_y - out_size_y/2)), (int(out_point_x + out_size_x/2), int(out_point_y + out_size_y/2)))\n","print(\"-\"*50)\n","print(f'inference time was: {(time.time() - t1) * 1000}  ms')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7MBjRWieq6V7"},"source":["# Bibliography\n","\n","[1] Jaegle, Andrew, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. \"Perceiver: General Perception with Iterative Attention.\" arXiv preprint arXiv:2103.03206 (2021).\n","\n","[2]\n","Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen et al. \"Pytorch: An imperative style, high-performance deep learning library.\" arXiv preprint arXiv:1912.01703 (2019)."]},{"cell_type":"markdown","metadata":{"id":"jGs9Ujm2qb0_"},"source":[""]}]}